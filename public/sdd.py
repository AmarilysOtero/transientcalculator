# -*- coding: utf-8 -*-
"""sdd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1obdFiMIeaGpz6CWdjn8ETuedQZB_5NQN

<h1> Implementado Sistemas Dinámicos Discretos en Python </h1>

<h3> Librerias usadas </h3>
"""

import networkx as nx
import numpy as np
from datetime import datetime
from time import sleep
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.animation
from matplotlib import rc
rc('animation', html='jshtml')
from PIL import Image
from numpy import asarray
import sys
import csv  
import pandas as pd

sys.setrecursionlimit(7000)

"""<h2> Código para resolver transient </h2>

<h3> Código para resolver los grafos </h3>
"""

def storePath(iteration, mat):
  mat.append(iteration)
  return 

def getOldDependencies(vector, initial_vec):
  total = 1
  for items in vector:
    total *= initial_vec[int(items)-1]
  return total


# recibe unas dependencias, vector inicial [1 y 0], un counter= 0 para las corridas, y una
# matriz de adjacency para ver resultados finales
def dynamicalSystem(dependencies, initial_vec, count, mat):
  # define dependencies
  # si todo nodo es igual a 1 o 0, terminamos
  if initial_vec.count(0) == len(initial_vec) or initial_vec.count(1) == len(initial_vec):
    # imprimimos el progreso hasta el momento.
    #print("Step #", count, initial_vec)
    return count
  else:
    # sleep(.25)
    # imprimimos el progreso hasta el momento.
    #print("Step #", count, initial_vec)

    # copy_vec va a servir como la próxima iteración de [1 y 0]
    copy_vec = [0 for x in range(len(initial_vec))]
    # corremos por cada dependencia y actualizamos los valores y los copiamos
    # a copy_vec
    for k, v in dependencies.items():
      total = 1
      for items in v:
        total *= initial_vec[int(items)-1]
      copy_vec[int(k) - 1] = total #getOldDependencies(v, initial_vec)
    
    # el valor global del vector se guarda y llamamos la función original
    # para el próximo paso de la recursión
    mat.append(copy_vec)
    return dynamicalSystem(dependencies, copy_vec, count+1, mat)

"""<h3> Área para correr código y tomar tiempo </h3>"""

def generateDepen(n, m, l):
  autogen = {}
  # dado el nodo, agrega la conexion correspondiente
  for i in range(1, n+m+l-1):
    if i not in autogen.keys():
      autogen[str(i)] = []
      autogen[str(i)].append(i+1)

    if i == n:
      autogen[str(i)].append(1)
      autogen[str(i)].sort()

    if i == m+n-1:

      autogen[str(i)].append(n)
      autogen[str(i)].sort()
    
    if i == n+m+l-2:
      autogen[str(i)] = [n+m-1]
      autogen[str(i)].sort()
  # devuelve el diccionario
  return autogen

# autogen[n+m+l-2].append(n+m-1)

# d = generateDepen(2, 3, 5)
# print(d)

def getTransient(a, b, c):

  if a == 1:
    #print("(" + str(a) + "," + str(b) + "," +str(c) + ") --> " + str(2*c + b - 2))
    return (a, b, c), 2*c + b - 2

  elif a == 2:
    #print("(" + str(a) + "," + str(b) + "," +str(c) + ") --> " + str(2*c + 2*b - 3))
    return (a, b, c), 2*c + 2*b - 3
    
  else:
    depen = generateDepen(a,b,c)
    initial_vec = [1 for x in range(a+b+c-2)]
    initial_vec[-1] = 0
    # copia global del progreso
    mat = [initial_vec]
    # llamamos la función dinámica
    r = dynamicalSystem(depen, initial_vec, 0, mat)
    return (a, b, c), r

    #print("(" + str(a) + "," + str(b) + "," +str(c) + ") --> " + str(r))


tupla, tran = getTransient(3, 43, 53)
tran

def isPrime(num):
  factors = []
  for i in range(1, num+1):
    if num % i == 0:
      factors.append(i)
  if len(factors) == 2:
    if num in factors and 1 in factors:
      return True
  return False

def nextPrime(num, bound):
  for i in range(num, bound+1):
    if isPrime(i):
      return i 
  return -1

def llamaTuplas(a, b, c, limit):

  coleccion = {}
  for nums in range(limit+100):

    if c == -1:
      return coleccion

    if c != -1:
      tupla, tran = getTransient(a, b, c)
      if tran not in coleccion.keys():
        coleccion[tran] = []
        coleccion[tran].append(tupla)
      else:
        coleccion[tran].append(tupla)

    #print(a, b, c)

    if c-b < 100: 
      c = nextPrime(c+1, limit)

    else:
      b = nextPrime(b+1, limit)
      c = nextPrime(b+1, limit)

  return coleccion

record1 = llamaTuplas(3, 5, 7, 500)
record2 = llamaTuplas(3, 71, 173, 500)

record1.update(record2)
# #record1
# for k in sorted(record.keys()):
#   format = str(k) + " --> " + ",".join(str(it) for it in record[k])
#   print(format)

"""<h2> Experimentando con resultados y análisis futuro"""

with open('data.csv', 'w', encoding='UTF8') as f:
  writer = csv.writer(f)

    # write the header
  writer.writerow(["Transient", "a", "b", "c"])
  
  for k in sorted(record1.keys()):
    for items in record1[k]:
      writer.writerow([k, items[0], items[1], items[2]])

x_cor, y_cor, z_cor = [],[],[]
for k, v in record1.items():
  for items in v:
    x_cor.append(items[1])
    y_cor.append(items[2])
    z_cor.append(int(k))

fig = plt.figure(figsize=(12, 12))
ax = fig.add_subplot(projection='3d')

ax.scatter(x_cor, y_cor, z_cor)
plt.show()

# ejemplo 1:

# dependencias generadas por Prof Colón
# depen = {1: [2], 2: [1, 3], 3: [4], 4: [2, 5], 5:[6], 6:[7], 7:[8], 8:[4]}
n = 2
m = 5
l = 7
depen = generateDepen(n,m,l)

# tomamos tiempo de ejecución
start_time = datetime.now()

# vector inicial con sus valores
initial_vec = [1 for x in range(n+m+l-2)]
initial_vec[-1] = 0
# copia global del progreso
mat = [initial_vec]
# llamamos la función dinámica
r = dynamicalSystem(depen, initial_vec, 0, mat)

mat.append([0 for x in range(len(mat[0]))])

# terminamos contando el tiempo
for i in range(r+1):
  print("step", i, "=", mat[i])
end_time = datetime.now()

print("Transient ->", r)


# imprimimos su tiempo de ejecución
print("----------------------------------")
print('Duration: {}'.format(end_time - start_time))

"""<h3> Extra ejercicios </h3>

En esta sección sería añadir ejercicios de dependencias, y las matrices/vectores iniciales como un banco de problemas. Si añaden uno, a mantener el formato initial_vec# y depen# (# siendo instancia). Luego debajo del ejercicio, hagan su corrida para verificar que están bien hechos.

Machine Learning Algorithm - Import Libraries
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import torchvision
import torchvision.transforms as transforms
import pandas as pd

"""Load Data"""

url = '/content/data.csv'
column_names = ['Transient', 'a', 'b', 'c']

raw_dataset = pd.read_csv(url)#, names=column_names,
                         # na_values='?', comment='\t',
                          #sep=' ', skipinitialspace=True)

raw_dataset

"""Show Example"""

dataset = raw_dataset.copy()
dataset.tail()

"""Prepare Data"""

# Drop missing values
dataset = dataset.dropna()

# Split dataset into inputs and labels
X = dataset.iloc[:, 1:].values
y = dataset.iloc[:, 0].values

# Normalize inputs
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

# Convert numpy arrays to tensors
X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32)


# Create dataset and dataloader
dataset = torch.utils.data.TensorDataset(X, y)
trainloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)

"""  Neural Network #1 - Linear Regression"""

class RegressionModel1(torch.nn.Module):
    def __init__(self, input_size):
        super(RegressionModel1, self).__init__()
        self.layer1 = torch.nn.Linear(input_size, 1)

    def forward(self, x):
        x = self.layer1(x)
        return x

nnet = RegressionModel1(3)

import torch.optim as optim
# Add L2 regularization to the loss function
l2_lambda = 0.01
criterion = torch.nn.MSELoss()
for param in nnet.parameters():
    if param.requires_grad:
        criterion = criterion + l2_lambda * torch.norm(param)

optimizer = torch.optim.SGD(nnet.parameters(), lr=0.01)

# Define an empty list to store the loss values
loss_list1 = []

for epoch in range(20):
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        #get inputs
        inputs, labels = data
        
        #zero parameter gradients
        optimizer.zero_grad()

        #forward, backward, optimize
        outputs = nnet(inputs)
        loss=criterion(outputs,labels.view(-1, 1))
        loss.backward()
        
        # Clip the gradients to prevent numerical instability
        torch.nn.utils.clip_grad_norm_(nnet.parameters(), max_norm=1)
        
        optimizer.step()

        #print
        running_loss += loss.item()
        if i % 12 == 11:
            print(f'[{epoch + 1}, {i+1:5d}] loss: {running_loss / 12:.3f}')
            running_loss=0.0
            
    loss_list1.append(running_loss / len(trainloader))
print('Finished Training')

"""Accuracy"""

correct = 0
total = 0
with torch.no_grad():
    for data in trainloader:
        # get inputs
        inputs, labels = data

        outputs = nnet(inputs)
        for i in range(len(labels)):
            if abs(outputs[i].item() - labels[i]) <= 5:
                correct += 1
            total += 1
acc1=100*correct/total
print(f'Approx. accuracy of the network: {100*correct/total:.2f} %')

"""Plot loss"""

plt.plot(loss_list1, 'r')
plt.tight_layout()
plt.grid(True,color='y')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.show()